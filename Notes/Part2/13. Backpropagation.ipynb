{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60ce5c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl, numpy as np\n",
    "from pathlib import Path\n",
    "from torch import tensor\n",
    "from fastcore.test import test_close\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "mpl.rcParams['image.cmap'] = 'gray'\n",
    "torch.set_printoptions(linewidth = 125)\n",
    "np.set_printoptions(linewidth = 125)\n",
    "\n",
    "MNIST_URL='https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/data/mnist.pkl.gz?raw=true'\n",
    "path_data = Path('data')\n",
    "path_data.mkdir(exist_ok=True)\n",
    "path_gz = path_data/'mnist.pkl.gz'\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "if not path_gz.exists(): urlretrieve(MNIST_URL, path_gz)\n",
    "\n",
    "# Open the compressed file and take out our data that we want then convert all of the separate datasets into tensors\n",
    "with gzip.open(path_gz, 'rb') as f:\n",
    "    ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1') \n",
    "    x_train, y_train, x_valid, y_valid = map(tensor, [x_train, y_train, x_valid, y_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ce84704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic architecture\n",
    "\n",
    "# rows and columns of our data - rows for each image and columns for each pixel of each image\n",
    "n, m = x_train.shape\n",
    "# c is the total outputs\n",
    "c = y_train.max() + 1 # possible outputs since we also have the number 0 included\n",
    "\n",
    "# number of hidden nodes\n",
    "nh = 50\n",
    "\n",
    "# Weights and Biases\n",
    "# Start with each pixel being an input - passing 1 image at a time\n",
    "w1 = torch.randn(m, nh)\n",
    "b1 = torch.zeros(nh)\n",
    "# Then we add a bias to all those hidden nodes\n",
    "# Finally we have our second layer which has a weight for all of the \n",
    "w2 = torch.randn(nh, 1)\n",
    "b2 = torch.zeros(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7de9da5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 50])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lin(x, w, b): \n",
    "    return x@w + b\n",
    "\n",
    "t = lin(x_valid, w1, b1)\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "02bd0e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return x.clamp_min(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "281e9f88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  1.4440,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 3.8119,  0.0000, 10.9341,  ...,  3.7241,  0.0000,  0.0000],\n",
       "        [11.3077,  0.0000,  6.6042,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        ...,\n",
       "        [15.4602,  0.0000,  6.8516,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 7.7826,  2.3167,  6.9597,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  8.5584,  ...,  6.1868,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = relu(t)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f8250596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 1])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model(xb):\n",
    "    l1 = lin(xb, w1, b1)\n",
    "    l2 = relu(l1)\n",
    "    return lin(l2, w2, b2)\n",
    "\n",
    "res = model(x_valid)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b9ea5d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2446c689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10000, 1]), torch.Size([10000]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "res.shape,y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a1037070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 1])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(res-y_valid[:, None]).shape\n",
    "# instead of taking the minus of both, we want to make sure that both follow the same rules\n",
    "# otherwise it will insert the unit axis of both\n",
    "# so we can add another dimension with none to make it work\n",
    "# so we take all rows and add 1 column with None\n",
    "\n",
    "# can also do res.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cb375514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 1])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train,y_valid = y_train.float(),y_valid.float()\n",
    "\n",
    "preds = model(x_train)\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8ae15313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1825.7338)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mse(output, target):\n",
    "    return (output[:, 0] - target).pow(2).mean()\n",
    "\n",
    "mse(preds, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "457e3213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 6 x$"
      ],
      "text/plain": [
       "6*x"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sympy import symbols, diff\n",
    "\n",
    "x, y = symbols('x y')\n",
    "y\n",
    "\n",
    "\n",
    "diff(3*x**2+9, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "edf6a4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_grad(inp, out, w, b):\n",
    "    # grad of matmul with respect to input\n",
    "    inp.g = out.g @ w.t()\n",
    "    w.g = (inp.unsqueeze(-1) * out.g.unsqueeze(1)).sum(0)\n",
    "    b.g = out.g.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "acdac5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_and_backward(inp, targ):\n",
    "    # forward pass:\n",
    "    l1 = lin(inp, w1, b1)\n",
    "    l2 = relu(l1)\n",
    "    out = lin(l2, w2, b2)\n",
    "    diff = out[:,0]-targ\n",
    "    # When we do the forward pass we calculate the loss by doing output - target\n",
    "    loss = diff.pow(2).mean()\n",
    "    \n",
    "    # backward pass - we contain the gradients as a property of the tensors\n",
    "    # \n",
    "    out.g = 2.*diff[:,None] / inp.shape[0]\n",
    "    lin_grad(l2, out, w2, b2)\n",
    "    l1.g = (l1>0).float() * l2.g\n",
    "    lin_grad(inp, l1, w1, b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3f296b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_and_backward(x_train, y_train)\n",
    "\n",
    "# Save for testing against later\n",
    "def get_grad(x): return x.g.clone()\n",
    "chks = w1,w2,b1,b2,x_train\n",
    "grads = w1g,w2g,b1g,b2g,ig = tuple(map(get_grad, chks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3f6756c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkgrad(x): return x.clone().requires_grad_(True)\n",
    "ptgrads = w12,w22,b12,b22,xt2 = tuple(map(mkgrad, chks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "155ab924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(inp, targ):\n",
    "    l1 = lin(inp, w12, b12)\n",
    "    l2 = relu(l1)\n",
    "    out = lin(l2, w22, b22)\n",
    "    return mse(out, targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b6c7fe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = forward(xt2, y_train)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "442bf8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refactoring as classes\n",
    "\n",
    "class Relu():\n",
    "    def __call__(self, inp):\n",
    "        self.inp = inp\n",
    "        self.out = inp.clamp_min(0.)\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self):\n",
    "        # multiplied due to the chain rule\n",
    "        self.inp.g = (self.inp > 0).float() * self.out.g\n",
    "        \n",
    "class Lin():\n",
    "    def __init__(self, w, b): \n",
    "        self.w, self.b = w, b\n",
    "\n",
    "    def __call__(self, inp):\n",
    "        self.inp = inp\n",
    "        self.out = lin(inp, self.w, self.b)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self):\n",
    "        self.inp.g = self.out.g @ self.w.t()\n",
    "        self.w.g = self.inp.t() @ self.out.g\n",
    "        self.b.g = self.out.g.sum(0)\n",
    "        \n",
    "class Mse():\n",
    "    def __call__(self, inp, targ):\n",
    "        self.inp,self.targ = inp,targ\n",
    "        self.out = mse(inp, targ)\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self):\n",
    "        self.inp.g = 2. * (self.inp.squeeze() - self.targ).unsqueeze(-1) / self.targ.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4db78f69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Model at 0x232be5e7150>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Model():\n",
    "    def __init__(self, w1, b1, w2, b2):\n",
    "        self.layers = [Lin(w1,b1), Relu(), Lin(w2,b2)]\n",
    "        self.loss = Mse()\n",
    "        \n",
    "    def __call__(self, x, targ):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return self.loss(x, targ)\n",
    "    \n",
    "    def backward(self):\n",
    "        self.loss.backward()\n",
    "        for l in reversed(self.layers): \n",
    "            l.backward()\n",
    "            \n",
    "model = Model(w1, b1, w2, b2)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a858d4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "10e10dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e5701a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(w2g, w2.g, eps=0.01)\n",
    "test_close(b2g, b2.g, eps=0.01)\n",
    "test_close(w1g, w1.g, eps=0.01)\n",
    "test_close(b1g, b1.g, eps=0.01)\n",
    "test_close(ig, x_train.g, eps=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9aaadf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Module():\n",
    "    def __call__(self, *args):\n",
    "        self.args = args\n",
    "        self.out = self.forward(*args)\n",
    "        return self.out\n",
    "\n",
    "    def forward(self): raise Exception('not implemented')\n",
    "    def backward(self): self.bwd(self.out, *self.args)\n",
    "    def bwd(self): raise Exception('not implemented')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d3c04eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# we can now import nn module from pytorch\n",
    "\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super().__init__()\n",
    "        # Import from nn.Module\n",
    "        self.w = torch.randn(n_in, n_out).requires_grad_()\n",
    "        self.b = torch.zeros(n_out).requires_grad_()\n",
    "        \n",
    "    def forward(self, inp): \n",
    "        return inp @ self.w + self.b\n",
    "     \n",
    "class Model(nn.Module):\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        super().__init__()\n",
    "        self.layers = [Linear(n_in, nh), nn.ReLU(), Linear(nh, n_out)]\n",
    "        \n",
    "    def __call__(self, x, targ):\n",
    "        for l in self.layers: \n",
    "            x = l(x)\n",
    "        return F.mse_loss(x, targ[:,None])       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dac05e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x):\n",
    "    # sum over the last dimension\n",
    "    # leavees the unit axis back in that state so we dont get a out of product dimension error\n",
    "    return (x.exp() / (x.exp().sum(-1, keepdim = True))).log()\n",
    "\n",
    "# is the simplified version\n",
    "def log_softmax(x): \n",
    "    return x - x.exp().sum(-1,keepdim=True).log()\n",
    "\n",
    "# larger numbers have a smaller precision - we want more precision so we want smaller numbers\n",
    "# by calculating the max of x and then we take the difference between the largest and every value\n",
    "# we can subtract a from all the exponents\n",
    "\n",
    "# LogSumExp Trick - allows us to have more accurate and smaller numbers\n",
    "def logsumexp(x):\n",
    "    # find the maximum of the last dimension\n",
    "    m = x.max(-1)[0]\n",
    "    return m + (x-m[:,None]).exp().sum(-1).log()\n",
    "\n",
    "def log_softmax(x): return x - x.logsumexp(-1,keepdim=True)\n",
    "\n",
    "# To optimise this we can simply index the log softmax values whislt also using the logsumexp trick\n",
    "# This will make it far faster allowing us to index the cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8b5b604d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross entropy - negative log likelihood loss is cross entropy loss\n",
    "# F.cross_entropy\n",
    "\n",
    "def nll(input, target): \n",
    "    return -input[range(target.shape[0]), target].mean()\n",
    "\n",
    "# function to find the index of the highest number is called torch.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb1ed73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate the accuracy is\n",
    "\n",
    "def accuracy(out, yb):\n",
    "    return (out.argmax(dim = 1) == yb).float().mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
