{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd46a65f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we want to make sure that at the start we have 0 mean and 1 stdevs\n",
    "# you can easily run out of gpu memory if you run cells on jupyter notebook\n",
    "# jupyter notebook stores the results of your previous few results and thus allows you to use\n",
    "# _ to get your previous one\n",
    "# if your previous output is a GPU tensor you must clean out those _\n",
    "\n",
    "# any cuda out of memory errors since the memory that was previously allocatd to your code will still be saved within the \n",
    "# traceback call of the error, it will keep the error problems saved\n",
    "\n",
    "# we must scale our weight matrices so that the standard deviation of our activations stay at 1\n",
    "# where 1 / root(number of inputs) - so we should scale our random numbers by 0.1\n",
    "# this is called the Xavier / Glorot initialisation\n",
    "# a variance is the average of how far away each point is from the mean \n",
    "# how far each point is on average from the mean\n",
    "import torch\n",
    "\n",
    "t = torch.tensor([1., 2., 3., 4., 5.])\n",
    "m = t.mean()\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5385924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(t - m).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d48f64e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# however to calculate this we have to either use the square root or the absolute value\n",
    "# whenever we are calculating the variance otherwise the positives and negatives will cancel out\n",
    "# using these functions (squaring to handle the positive and negative values)\n",
    "# is far easier within derivations and other calculations so we use this one as teh variance\n",
    "# however it is particularly sensitive to outliers and thus we can take the sqrt of variance\n",
    "# to get the standard deviation\n",
    "# it will alwasy be on the same scale as the original data\n",
    "(t - m).pow(2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd69b9d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sqrt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m sqrt((t \u001b[38;5;241m-\u001b[39m m)\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sqrt' is not defined"
     ]
    }
   ],
   "source": [
    "sqrt((t - m).pow(2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed4f5deb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.), tensor(2.))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(t-m).pow(2).mean(), (t*t).mean() - (m*m)\n",
    "the mean of the squared data points minus the squared mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4819eddf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'v' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Covariance\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Tells you how much two things vary with respect to eachother\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m cov \u001b[38;5;241m=\u001b[39m (t\u001b[38;5;241m*\u001b[39mv)\u001b[38;5;241m.\u001b[39mmean() \u001b[38;5;241m-\u001b[39m t\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m*\u001b[39mv\u001b[38;5;241m.\u001b[39mmean(); cov\n",
      "\u001b[1;31mNameError\u001b[0m: name 'v' is not defined"
     ]
    }
   ],
   "source": [
    "# Covariance\n",
    "# Tells you how much two things vary with respect to eachother\n",
    "cov = (t*v).mean() - t.mean()*v.mean(); cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85df1ad2",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1411608723.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[9], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    pearson correlation coefficient is the scaled version of covariance\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pearson correlation coefficient is the scaled version of covariance\n",
    "# covaraince divided by the standard deviation of both values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb9f8870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.04328734785318375, 100.3774040222168)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# y[i] = sum([c*d for c,d in zip(a[i], x)])\n",
    "# y[i] = (a[i]*x).sum()\n",
    "\n",
    "# At the very beginning, our x vector has a mean of roughly 0. and a standard deviation of roughly 1. (since we picked it that way).\n",
    "\n",
    "mean,sqr = 0.,0.\n",
    "\n",
    "for i in range(100):\n",
    "    x = torch.randn(100)\n",
    "    a = torch.randn(512, 100)\n",
    "    \n",
    "    y = a @ x\n",
    "    mean += y.mean().item()\n",
    "    sqr  += y.pow(2).mean().item()\n",
    "    \n",
    "mean/100,sqr/100\n",
    "\n",
    "# as long as both are indepent they will give a mean and std of 0 and 1\n",
    "# so by using this 1/ root (number of inputs) this means that our stdevs of 1 that are added up will all converge to 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbf303dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inplace' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 23\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# if we do a relu this means that we do not have a 0 mean or 1 stdevs\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# this means htat we must use kaiming init\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# instead of all of them being added all our means and variances and being changed\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m \n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# allows us to normalise inputs\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;129m@inplace\u001b[39m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransformi\u001b[39m(b): b[xl] \u001b[38;5;241m=\u001b[39m [(TF\u001b[38;5;241m.\u001b[39mto_tensor(o)\u001b[38;5;241m-\u001b[39mxmean)\u001b[38;5;241m/\u001b[39mxstd \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m b[xl]]\n\u001b[0;32m     25\u001b[0m tds \u001b[38;5;241m=\u001b[39m dsd\u001b[38;5;241m.\u001b[39mwith_transform(transformi)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'inplace' is not defined"
     ]
    }
   ],
   "source": [
    "# if we do a relu this means that we do not have a 0 mean or 1 stdevs\n",
    "# this means htat we must use kaiming init\n",
    "# instead of all of them being added all our means and variances and being changed\n",
    "# 1 / root(2 / n)\n",
    "\n",
    "# now that we know which initialisation functions we can use how will we apply them\n",
    "\n",
    "# model.apply applying init noramlly distributed random numbers\n",
    "# _ at the end of a function will change in place the method\n",
    "# however we can only do this if the model is a convolutional or linear layer\n",
    "# apply also returns the model meaning that we can simply\n",
    "# pass our model to the momentum learner with the dataloader, cross entorpy learning rate and callbacks\n",
    "# forcing our model to find ways of compressing intelligently\n",
    "# by decreasing grid size, we also get a lower amount of compute necessary\n",
    "# we also have to make sure we normalise our input matrix\n",
    "\n",
    "# for this we can use batchtransform callback\n",
    "# this will allow us to transform every single batch\n",
    "\n",
    "# Problem here is we are putting our values through a relu\n",
    "\n",
    "# allows us to normalise inputs\n",
    "@inplace\n",
    "def transformi(b): b[xl] = [(TF.to_tensor(o)-xmean)/xstd for o in b[xl]]\n",
    "tds = dsd.with_transform(transformi)\n",
    "dls = DataLoaders.from_dd(tds, bs, num_workers=4)\n",
    "xb,yb = next(iter(dls.train))\n",
    "\n",
    "# relu is incompatible with the idea of normalised data\n",
    "class GeneralRelu(nn.Module):\n",
    "    def __init__(self, leak=None, sub=None, maxv=None):\n",
    "        super().__init__()\n",
    "        self.leak,self.sub,self.maxv = leak,sub,maxv\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = F.leaky_relu(x,self.leak) if self.leak is not None else F.relu(x)\n",
    "        if self.sub is not None: x -= self.sub\n",
    "        if self.maxv is not None: x.clamp_max_(self.maxv)\n",
    "        return x\n",
    "    \n",
    "# by subtracting values from our relu we can pull everything so that we can then ahve a mean of 0\n",
    "# we can also use a leaky relu to not have negatives totaly flat / truncated\n",
    "# but these values can be decreased by a certain amount\n",
    "\n",
    "# leakiness of a relu means the slope instead of being a simple truncation \n",
    "# e.g leak of 0.1\n",
    "# we can also subtract a certain value that  finds our values our mean of 0\n",
    "\n",
    "# kaiming normal in pytorch provides us with the ability to give it a leaky vlaue\n",
    "# we also want to minimise the amount of dead units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb4fa92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSUV - layer wise sequential unit variance\n",
    "# allows us to do weight initialisation for deep learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "505839f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 11\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Batch normalisation\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# using batch normalisation allows you to train models so much faster\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# by using normalisation during training for each mini-batch then this allows us to use much higher learning rates and not care as much\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# variance for each input too\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# normalize data by subtracting the mean and dividing by the variance\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mLayerNorm\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dummy, eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m):\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "# Batch normalisation\n",
    "# using batch normalisation allows you to train models so much faster\n",
    "# by using normalisation during training for each mini-batch then this allows us to use much higher learning rates and not care as much\n",
    "# about learning rates / initialisation\n",
    "\n",
    "# multiply things by 1 and add 0\n",
    "# forward function\n",
    "# mean activation for each input - channel, height, width\n",
    "# variance for each input too\n",
    "# normalize data by subtracting the mean and dividing by the variance\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dummy, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.mult = nn.Parameter(tensor(1.))\n",
    "        self.add  = nn.Parameter(tensor(0.))\n",
    "\n",
    "    def forward(self, x):\n",
    "        m = x.mean((1,2,3), keepdim=True)\n",
    "        v = x.var ((1,2,3), keepdim=True)\n",
    "        x = (x-m) / ((v+self.eps).sqrt())\n",
    "        return x*self.mult + self.add\n",
    "    \n",
    "# by adding epsilon this means that we will not be dividing by 0 and we normalise the minibatch / layer that we decide to put this into\n",
    "# if we want anything other than unit variance / unit mean then we can change these parameters\n",
    "# and we can learn these parameters\n",
    "# they may not actually be normalisation\n",
    "# it normalises for initial layers\n",
    "# however later on it actually allows it to have 2 numbers to create any distribution of outputs it wants\n",
    "# this allows the neural network to learn the distribution that best suits this dataset and thus be far faster at producing accurate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d532fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# however it also adds a lot of complexity within our model, \n",
    "# Batchnormalisation\n",
    "# we have a vector of additions and a vector of multiples\n",
    "# this is why we have to pass in the number of filters that we have\n",
    "# we take the average of evverything in the batch\n",
    "\n",
    "# we generate arrays of these and 1 per filter\n",
    "# then we use .lerp_()\n",
    "# this allows you to have a sliding scale of a value between 0 and 1 of how much you want of either first and second vlaue combined\n",
    "# exponentially weighted moving average\n",
    "# and this updaets our values\n",
    "# then during inference we use the calcualted values\n",
    "# batch_norm uses register_buffer\n",
    "# batch norm is the average of hte input batch - 1 noramlisation value over 1 channel\n",
    "# layer norm evrages over teh channel - from the input\n",
    "\n",
    "# group norm groups a part of a channel together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ac87ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accelerated sgd\n",
    "\n",
    "# weight decay is similar / the same to l2 regularisation\n",
    "# means add the square of the weights to the loss function\n",
    "# by adding hte square of the weights to teh loss function - this means that \n",
    "# by using momentum we want to be able to follow the general direction of hte curve since the noise will cancel out itself\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8087681",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SGD' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mMomentum\u001b[39;00m(SGD):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, params, lr, wd\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.\u001b[39m, mom\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m):\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(params, lr\u001b[38;5;241m=\u001b[39mlr, wd\u001b[38;5;241m=\u001b[39mwd)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SGD' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "class Momentum(SGD):\n",
    "    def __init__(self, params, lr, wd=0., mom=0.9):\n",
    "        super().__init__(params, lr=lr, wd=wd)\n",
    "        self.mom=mom\n",
    "\n",
    "    def opt_step(self, p):\n",
    "        # for each parameter we must find out the moving average of gradients\n",
    "        # so we can set it to 0 initially\n",
    "        # then if we have it awe add what it used to be * momentum\n",
    "        # and then we use a lerp p / (1 - p) of the actual gradient vs momentum\n",
    "        if not hasattr(p, 'grad_avg'): \n",
    "            p.grad_avg = torch.zeros_like(p.grad)\n",
    "        p.grad_avg = p.grad_avg * self.mom + p.grad*(1-self.mom)\n",
    "        p -= self.lr * p.grad_avg\n",
    "        \n",
    "# the smaller the batch size the better - this would allow us to give the nerual network\n",
    "# to update as much as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecfe0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMS Prop\n",
    "# updates the optimisation step with instead of using the lerp of the gradietn\n",
    "# we use the lerp of the gradient^2\n",
    "# if the gradient is moving around allover the place we don't know what it is\n",
    "# if we are confident on the gradient it will do a big step\n",
    "\n",
    "# RMS Prop + Momentum is what adam is\n",
    "# Adam is rms prop + momentum\n",
    "\n",
    "# beta 1 is momentum \n",
    "# beta 2 is the momentum for the squares\n",
    "\n",
    "# now we can implement schedulers to optimise our learning rate - they find the optimal one by themselves\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
