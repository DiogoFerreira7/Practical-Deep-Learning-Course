{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b9b1fad",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'miniai'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optim\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mminiai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfastprogress\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m progress_bar,master_bar\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'miniai'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f85ac225",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'miniai'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn,tensor\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset,load_dataset_builder\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mminiai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mminiai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'miniai'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293623fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading a dataloader - using a batch sampler and 4 workers\n",
    "dls = DataLoaders.from_dd(tds, bs, num_workers=4)\n",
    "dt = dls.train\n",
    "xb,yb = next(iter(dt))\n",
    "xb.shape,yb[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb5f0006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a very standard basic learner, which is not very flexible\n",
    "# If we want to be able to change models quickly, test different, learning rates differently\n",
    "# \n",
    "\n",
    "class Learner:\n",
    "    # model, dataloader, loss function, learning rate, optimiser\n",
    "    def __init__(self, model, dls, loss_func, lr, opt_func=optim.SGD):\n",
    "        # storing all the attributes\n",
    "        fc.store_attr()\n",
    "\n",
    "    def one_batch(self):\n",
    "        self.xb,self.yb = to_device(self.batch)\n",
    "        self.preds = self.model(self.xb)\n",
    "        self.loss = self.loss_func(self.preds, self.yb)\n",
    "        if self.model.training:\n",
    "            self.loss.backward()\n",
    "            self.opt.step()\n",
    "            self.opt.zero_grad()\n",
    "        with torch.no_grad(): self.calc_stats()\n",
    "\n",
    "    def calc_stats(self):\n",
    "        acc = (self.preds.argmax(dim=1)==self.yb).float().sum()\n",
    "        self.accs.append(acc)\n",
    "        n = len(self.xb)\n",
    "        self.losses.append(self.loss*n)\n",
    "        self.ns.append(n)\n",
    "    \n",
    "    # loop through each batch in a data learner and call the model\n",
    "    def one_epoch(self, train):\n",
    "        self.model.training = train\n",
    "        dl = self.dls.train if train else self.dls.valid\n",
    "        for self.num,self.batch in enumerate(dl): self.one_batch()\n",
    "        n = sum(self.ns)\n",
    "        print(self.epoch, self.model.training, sum(self.losses).item()/n, sum(self.accs).item()/n)\n",
    "    \n",
    "    def fit(self, n_epochs):\n",
    "        self.accs,self.losses,self.ns = [],[],[]\n",
    "        self.model.to(def_device)\n",
    "        # hardcoding a single learning rate\n",
    "        self.opt = self.opt_func(self.model.parameters(), self.lr)\n",
    "        self.n_epochs = n_epochs\n",
    "        for self.epoch in range(n_epochs):\n",
    "            self.one_epoch(True)\n",
    "            with torch.no_grad(): self.one_epoch(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb484ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you use @property decorator this means that you don't have to call the function\n",
    "# you just call the property and it will output the value\n",
    "\n",
    "# everytime we have a callback the callback points to this with_cbs() function\n",
    "# which has a __call__() method taht returns the _f function which has 2 callbacks before and after the function being ran\n",
    "# this allows us to do things before and after each learner method\n",
    "class with_cbs:\n",
    "    def __init__(self, nm): self.nm = nm\n",
    "    def __call__(self, f):\n",
    "        def _f(o, *args, **kwargs):\n",
    "            try:\n",
    "                o.callback(f'before_{self.nm}')\n",
    "                f(o, *args, **kwargs)\n",
    "                o.callback(f'after_{self.nm}')\n",
    "            except globals()[f'Cancel{self.nm.title()}Exception']: pass\n",
    "            finally: o.callback(f'cleanup_{self.nm}')\n",
    "        return _f\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "557d04d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic callbacks learner\n",
    "\n",
    "# self.callback('before_fit')\n",
    "# passes in a list of our callbacks and the method name - method name being 'before_fit'\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "082188de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cbs(cbs, method_nm, learn=None):\n",
    "    for cb in sorted(cbs, key=attrgetter('order')):\n",
    "        method = getattr(cb, method_nm, None)\n",
    "        if method is not None: method(learn)\n",
    "            \n",
    "# will list our callbacks and order them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e740b69",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Callback' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Very simple callback with the method name written as a stringa and then will run them\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# get attr\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCompletionCB\u001b[39;00m(Callback):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbefore_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, learn): \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcount \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mafter_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m, learn): \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcount \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Callback' is not defined"
     ]
    }
   ],
   "source": [
    "# Very simple callback with the method name written as a stringa and then will run them\n",
    "# get attr\n",
    "class CompletionCB(Callback):\n",
    "    def before_fit(self, learn): self.count = 0\n",
    "    def after_batch(self, learn): self.count += 1\n",
    "    def after_fit(self, learn): print(f'Completed {self.count} batches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55722e2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Callback' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCompletionCB\u001b[39;00m(Callback):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbefore_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, learn): \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcount \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mafter_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m, learn): \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcount \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Callback' is not defined"
     ]
    }
   ],
   "source": [
    "class CompletionCB(Callback):\n",
    "    def before_fit(self, learn): self.count = 0\n",
    "    def after_batch(self, learn): self.count += 1\n",
    "    def after_fit(self, learn): print(f'Completed {self.count} batches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "249ba75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method is run at the end of the class\n",
    "# it is responsible for looping through all of the callbacks in the sorted order that they have within their respective classe\n",
    "# and then after we sorte them every single one will try to get the attribute taht we listed called method name\n",
    "# this method name will run using method() as a callback as something that should be done\n",
    "# now we can specify whatever kind of callbacks we want, before_fit, after_fit, before_batch, after_batch, after_fit, after_epcoh\n",
    "# or any period after we do a specific thing any of these can be called\n",
    "def run_cbs(cbs, method_nm, learn=None):\n",
    "    for cb in sorted(cbs, key=attrgetter('order')):\n",
    "        method = getattr(cb, method_nm, None)\n",
    "        if method is not None: \n",
    "            method(learn)\n",
    "            \n",
    "# each method has a self.callback call running before and after it has done it's main loop\n",
    "# callbacks can call exceptions incase we decide to not run batches / epochs depending on specific rules that we applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ce77b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metric:\n",
    "    def __init__(self): \n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self): \n",
    "        self.vals,self.ns = [],[]\n",
    "        \n",
    "    def add(self, inp, targ=None, n=1):\n",
    "        self.last = self.calc(inp, targ)\n",
    "        self.vals.append(self.last)\n",
    "        self.ns.append(n)\n",
    "        \n",
    "    @property\n",
    "    def value(self):\n",
    "        ns = tensor(self.ns)\n",
    "        return (tensor(self.vals)*ns).sum()/ns.sum()\n",
    "    \n",
    "    def calc(self, inps, targs): \n",
    "        return inps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51b88215",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accuracy(Metric):\n",
    "    def calc(self, inps, targs): \n",
    "        return (inps==targs).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77963256",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torcheval'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Torcheval means it has evaluation metrics for the pytroch library\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorcheval\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MulticlassAccuracy, Mean\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Creating a context manager that will be able to handle the callbacks being called\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# everything before the yield will be called before and anything after teh yield\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# after the code is finished being donep\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;129m@contextmanager\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcb_ctx\u001b[39m(\u001b[38;5;28mself\u001b[39m, nm):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torcheval'"
     ]
    }
   ],
   "source": [
    "# Torcheval means it has evaluation metrics for the pytroch library\n",
    "from torcheval.metrics import MulticlassAccuracy, Mean\n",
    "\n",
    "# Creating a context manager that will be able to handle the callbacks being called\n",
    "# everything before the yield will be called before and anything after teh yield\n",
    "# after the code is finished being donep\n",
    "@contextmanager\n",
    "def cb_ctx(self, nm):\n",
    "    try:\n",
    "        self.callback(f'before_{nm}')\n",
    "        yield\n",
    "        self.callback(f'after_{nm}')\n",
    "    except globals()[f'Cancel{nm.title()}Exception']: pass\n",
    "    finally: self.callback(f'cleanup_{nm}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5efa858a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# __getattr__ then it will call this method if it doesn't exist\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# instead get a callback passing in that name\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# this means that we can handle these 4 as callbacks\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# and now we can \u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "# __getattr__ then it will call this method if it doesn't exist\n",
    "# instead get a callback passing in that name\n",
    "# this means that we can handle these 4 as callbacks\n",
    "# and now we can \n",
    "if self.training:\n",
    "    self.backward()\n",
    "    self.step()\n",
    "    self.zero_grad()\n",
    "    \n",
    "def __getattr__(self, name):\n",
    "    if name in ('predict','get_loss','backward','step','zero_grad'): \n",
    "        return partial(self.callback, name)\n",
    "    raise AttributeError(name)\n",
    "    \n",
    "class TrainCB(Callback):\n",
    "    def __init__(self, n_inp=1): self.n_inp = n_inp\n",
    "    def predict(self, learn): learn.preds = learn.model(*learn.batch[:self.n_inp])\n",
    "    def get_loss(self, learn): learn.loss = learn.loss_func(learn.preds, *learn.batch[self.n_inp:])\n",
    "    def backward(self, learn): learn.loss.backward()\n",
    "    def step(self, learn): learn.opt.step()\n",
    "    def zero_grad(self, learn): learn.opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfb6cdb8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Callback' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 11\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# We can then override our learner\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# for example for zero_grad() instead of zeroing the gradients we can multiply them by a specific momentum\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# this means that our previous gradients would still influence our given values, they would just reduce it a bit\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# learning rate finders are useful to find the steepest gradient that we can have\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# each bath we multiply the learning rate and continuously increase it to find out how it works\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mLRFinderCB\u001b[39;00m(Callback):\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lr_mult\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.3\u001b[39m): \n\u001b[0;32m     13\u001b[0m         fc\u001b[38;5;241m.\u001b[39mstore_attr()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Callback' is not defined"
     ]
    }
   ],
   "source": [
    "# We can then override our learner\n",
    "# for example for zero_grad() instead of zeroing the gradients we can multiply them by a specific momentum\n",
    "# this means that our previous gradients would still influence our given values, they would just reduce it a bit\n",
    "# pytorch add the gradietns to teh existing gradients\n",
    "\n",
    "# we can find a learning rate finder\n",
    "# learning rate finders are useful to find the steepest gradient that we can have\n",
    "# each bath we multiply the learning rate and continuously increase it to find out how it works\n",
    "\n",
    "\n",
    "class LRFinderCB(Callback):\n",
    "    def __init__(self, lr_mult=1.3): \n",
    "        fc.store_attr()\n",
    "    \n",
    "    def before_fit(self, learn):\n",
    "        self.lrs, self.losses = [],[]\n",
    "        self.min = math.inf\n",
    "\n",
    "    def after_batch(self, learn):\n",
    "        # do not need to do any validatoin in learning rate finding\n",
    "        if not learn.training: \n",
    "            raise CancelEpochException()\n",
    "        \n",
    "        self.lrs.append(learn.opt.param_groups[0]['lr'])\n",
    "        loss = to_cpu(learn.loss)\n",
    "        # append the current loss\n",
    "        self.losses.append(loss)\n",
    "        \n",
    "        # see how we want to modify the learning rate based on how the loss is right now\n",
    "        if loss < self.min: \n",
    "            self.min = loss\n",
    "        if loss > self.min*3: \n",
    "            raise CancelFitException()\n",
    "        # change the learning rate by the multiplier\n",
    "        for g in learn.opt.param_groups: \n",
    "            g['lr'] *= self.lr_mult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36f77d6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "torch.optim.lr_scheduler\n",
    "\n",
    "# a learning rate scheduler will decay the learning rate of the learner after every time you set step\n",
    "\n",
    "# Actionvation stats allow us to look inside our models and diagnose how our model is doing\n",
    "# torch.manual_seed(seed)\n",
    "# randm.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9716b568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# High learning rates can cause the model to not train well\n",
    "# we do not want activations close to 0 otherwise they will all just be dead units\n",
    "# mean of 0 as long as they have a standard deviation around 1 or bigger we do not want a std of 0\n",
    "\n",
    "# Hooks allow you to look inside our model and visualise how it is training\n",
    "# they will be attached to a specific layer that you attach them to and then be called whenever that layer is cross either during the \n",
    "# forward pass or a backward pass depending on which kind of hook they are\n",
    "\n",
    "# layer number, module, inputs and outputs\n",
    "\n",
    "# go through each layer of the module and register_forward_hook\n",
    "# after you register the forward hook which should be a partial(hook_function_name, layer index)\n",
    "# it will call it whenever you run it\n",
    "\n",
    "# hooks are a particular kind of callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb7e521d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Hooks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Hook classes would simplify this\u001b[39;00m\n\u001b[0;32m      2\u001b[0m  \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# make the hooks class a context manager \u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Hooks(model, append_stats) \u001b[38;5;28;01mas\u001b[39;00m hooks:\n\u001b[0;32m      5\u001b[0m     fit(model)\n\u001b[0;32m      6\u001b[0m     fig,axs \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m4\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Hooks' is not defined"
     ]
    }
   ],
   "source": [
    "# Hook classes would simplify this\n",
    " \n",
    "# make the hooks class a context manager \n",
    "with Hooks(model, append_stats) as hooks:\n",
    "    fit(model)\n",
    "    fig,axs = plt.subplots(1,2, figsize=(10,4))\n",
    "    for h in hooks:\n",
    "        for i in 0,1: axs[i].plot(h.stats[i])\n",
    "    plt.legend(range(6));\n",
    "\n",
    "    # classs for context manager\n",
    "    # __enter__() - when it hits the with statement\n",
    "    # __exit__() - when it finishes\n",
    "class Hooks(list):\n",
    "    def __init__(self, ms, f): super().__init__([Hook(m, f) for m in ms])\n",
    "    def __enter__(self, *args): return self\n",
    "    def __exit__ (self, *args): self.remove()\n",
    "    def __del__(self): self.remove()\n",
    "    def __delitem__(self, i):\n",
    "        self[i].remove()\n",
    "        super().__delitem__(i)\n",
    "    def remove(self):\n",
    "        for h in self: h.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0140a8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
