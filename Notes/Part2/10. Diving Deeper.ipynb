{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20e9a683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can use spherical linear interpolation and make the image convert from one bit to another one\n",
    "# Unet takes the image and predits the noise within the image\n",
    "# then it will compare its prediction to the actual noise that \n",
    "# VAE latents are just a computational shortcut that makes things preprocessing faster \n",
    "\n",
    "# CLIP embeddings using an image encoder and a text encoder create embeddings of images and text that are as similar as possible - we can do this by training images with alt tags\n",
    "    # images to feature vector\n",
    "    # text encoder to features\n",
    "    # large dot products for diagonals and near 0 for non diagonal dot products\n",
    "    # text encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e313d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Progressive Distillation for Fast Sampling of Diffusion Models\n",
    "# Teacher -> student network\n",
    "\n",
    "# Putting latent image through a Unet B -> this should compare the intermediate output to complete images instantaneously\n",
    "# Unet B\n",
    "# Teacher model that has already been trained (Diffusion model) - take in the noise image\n",
    "# put it through two time steps and put it through \n",
    "\n",
    "\n",
    "# unit B\n",
    "# from the noise to timestep 2\n",
    "# this student model learns of two goes from the student model again and this is recursively done\n",
    "# so that each student doubles the amount of work and does two timesteps - the new Unet learns to go through multiple steps at a time\n",
    "\n",
    "#Classifier Free Guided Diffusion models\n",
    "# CLIP gives us an embedding from this input and we put it into the Unet along with the empty prompt into the encoder\n",
    "# this ranodm noise image is added together and we take the weighted average\n",
    "# however this is awkward and we can skip it by\n",
    "# using student teacher distillation by passing it the guidance\n",
    "\n",
    "# student model has noise, prompt, guidance scale\n",
    "# Teacher - student model reducing hte amount of time steps that we need to be done here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "678acfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image editing with AI\n",
    "\n",
    "# take a pretrained diffusion model\n",
    "# use an embedding from the CLIP \n",
    "\n",
    "# fine tune the embedding to make the diffusion model output something as similar to the input image\n",
    "# after teh embedding is optimised\n",
    "\n",
    "# you fine tune the entire model \n",
    "# then you interpolate the weighted average of the targett embedding and the optimised embedding and get teh output of the fine tuned diffusion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48269fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
