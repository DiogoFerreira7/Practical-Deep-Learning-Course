{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d2be3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing NLP with Hugging Face library\n",
    "\n",
    "# Pre trained model, provides pre fit parameters which are already quite good at the task at hand.\n",
    "# Fine tuning allows us to tune the end parameters to make it fit uniquely towards our given task that we want\n",
    "\n",
    "# Transformers take phenomenal advantage of modern accelerators, they take chunks of text and delete a couple of words that the model has to predict\n",
    "# Earlier layers are unlikely that you will need to change much since they are very basic simple detections "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3964b6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification of NLP\n",
    "\n",
    "# Sentiment analysis\n",
    "# Author identification \n",
    "# Legal Discovery\n",
    "# Organising and classifying by topic\n",
    "# Triaging inbound emails - where they should be going and what they want to be done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54b78da0",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16880\\3388966243.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Hugging Face -> converting the pandas dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pandas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "# Hugging Face -> converting the pandas dataframe\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "ds = Dataset.from_pandas(df)\n",
    "\n",
    "# Tokenisation - split each text up into tokens\n",
    "# Numericalistaion - convert each token into a number\n",
    "\n",
    "# deberta-v3 is a quite good useful model\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# AutoTokenizer -> creates a tokeniser appropriate for a given model\n",
    "tokz = AutoTokenizer.from_pretrained(model_nm)\n",
    "tokz.tokenizer(\"Hello this is a test sentence\")\n",
    "\n",
    "# This will tokenise everything to fit them model that we pass in\n",
    "# We can then do this in parallel and save a lot of time\n",
    "\n",
    "def tok_func(x): return tokz(x[\"input\"])\n",
    "\n",
    "toks_ds = ds.map(tok_func, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c8b7bde",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16880\\2925582316.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# When you have a dataset with too many points use a sample to plot far less points and get a random sample from it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_printoptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprecision\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msuppress\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;31m# np.corrcoef() - to give us r\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# r is very very sensitive to outliers, by removing noise you get a far higher r\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Long documents with transformer based approaches struggle with large amounts of data being passed into it\n",
    "\n",
    "# Pearson correlation coefficient between -1 and +1 and it is the most widely used measure of the degreee of relationship between two variables\n",
    "# When you have a dataset with too many points use a sample to plot far less points and get a random sample from it\n",
    "\n",
    "np.set_printoptions(precision = 2, suppress = True)\n",
    "# np.corrcoef() - to give us r\n",
    "# r is very very sensitive to outliers, by removing noise you get a far higher r\n",
    "# or if you get a couple of rows wrong on kaggle, then those little amounts of outliers will damage your entire potential score\n",
    "\n",
    "# batch size - the the nubmer of rows that we possibly have, by having a larger batch size we will have more rows and thus be able to parallelise far more data at once\n",
    "# mini batch - size of the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e901293",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'learn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16880\\3920389796.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# can save amodel using fastai using\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"1epoch\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"1epoch\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# unfreezing layers once at a time in NLP makes a large difference whereas in CV we can simply unfreeze the layers we want\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'learn' is not defined"
     ]
    }
   ],
   "source": [
    "# can save amodel using fastai using \n",
    "learn.save(\"1epoch\")\n",
    "learn.load(\"1epoch\")\n",
    "\n",
    "# unfreezing layers once at a time in NLP makes a large difference whereas in CV we can simply unfreeze the layers we want\n",
    "# all together at the same time whenever we begin fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e8337c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self supervised models in nlp can be used for pre-trained models that can then be used for transfer learning\n",
    "# By pretraining it with a large corpus it will better understand the language style, structure of the text classification dataset adn thus can perform better when fine-tuned\n",
    "\n",
    "# Steps to preparing your data for a langauge model\n",
    "# Tokenisation\n",
    "# Numericalisation\n",
    "# Language model dataloader\n",
    "\n",
    "# by replacing some niche words with unknown word tokens we make sure that the embedding matrix is not too large, does not have its memory usage increased\n",
    "# and it speeds up training\n",
    "# therefore we can set a min_freq of the word in the corpus for it to be assigned a token and finally a value according to it\n",
    "\n",
    "# Since documetns have variable sizes, padding is needed to collate the batch.\n",
    "# Cropping or squishing the batch data will negatively affect teh training and will not make sense in this context.\n",
    "# embedding size * vocab size where embedding size will be the number defining the number of latent factors of the tokens\n",
    "\n",
    "# perplexity is a common metric in NLP - how confidently a model will predict the following word\n",
    "# text generation models are mostly alwasy used within the fine tuning process of a classification model and thus\n",
    "# they will always be ahead.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
